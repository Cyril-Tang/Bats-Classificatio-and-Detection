@In Book{bottou2012stochastic,
author = {Bottou, Leon},
title = {Stochastic Gradient Descent Tricks},
series = {Lecture Notes in Computer Science (LNCS)},
booktitle = {Neural Networks, Tricks of the Trade, Reloaded},
year = {2012},
month = {January},
abstract = {The first chapter of Neural Networks, Tricks of the Trade strongly advocates the stochastic back-propagation method to train neural networks. This is in fact an instance of a more general technique called stochastic gradient descent. This chapter provides background material, explains why SGD is a good learning algorithm when the training set is large, and provides useful recommendations. This chapter appears in the “reloaded” edition of the tricks book (springer). It completes the material presented in the initial chapter "Efficient Backprop".},
publisher = {Springer},
url = {https://www.microsoft.com/en-us/research/publication/stochastic-gradient-tricks/},
pages = {430-445},
volume = {7700},
edition = {Neural Networks, Tricks of the Trade, Reloaded},
}